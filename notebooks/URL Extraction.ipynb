{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "metadata": {
        "id": "owT9s_tDEPZj",
        "outputId": "0c4d9c0b-d383-492e-fd61-24018d53ebeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein\n",
        "!pip install tldextract\n",
        "!pip install python-whois"
      ],
      "metadata": {
        "id": "fwbsiyrmDW2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import socket\n",
        "import whois\n",
        "import Levenshtein\n",
        "import math\n",
        "import requests\n",
        "import urllib.parse\n",
        "from urllib.parse import urlparse\n",
        "from os.path import splitext\n",
        "from tldextract import tldextract\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "4-wXGGt7DRTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HINTS = ['wp', 'login', 'includes', 'admin', 'content', 'site', 'images', 'js', 'alibaba', 'css', 'myaccount', 'dropbox', 'themes', 'plugins', 'signin', 'view', 'invoice', 'new', 'message', 'required', 'verification']\n",
        "\n",
        "allBrand = []\n",
        "with open(\"/gdrive/MyDrive/Colab Notebooks/URLSETs /allbrands.txt\", \"r\") as file:\n",
        "    for line in file:\n",
        "        allBrand.append(line.strip())"
      ],
      "metadata": {
        "id": "hebWqBKKDeLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def url_length(url):\n",
        "    return len(url)\n",
        "\n",
        "def get_domain_from_url(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    if parsed_url.netloc:\n",
        "        return parsed_url.netloc.split(\":\")[0]\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def hostname_length(domain):\n",
        "    return len(domain)\n",
        "\n",
        "def count_tilde(url):\n",
        "    return url.count('~')\n",
        "\n",
        "def count_exclamation(url):\n",
        "    return url.count('!')\n",
        "\n",
        "def count_at(url):\n",
        "    return url.count('@')\n",
        "\n",
        "def count_hash(url):\n",
        "    return url.count('#')\n",
        "\n",
        "def count_dollar(url):\n",
        "    return url.count('$')\n",
        "\n",
        "def count_percentage(url):\n",
        "    return url.count('%')\n",
        "\n",
        "def count_caret(url):\n",
        "    return url.count('^')\n",
        "\n",
        "def count_and(url):\n",
        "    return url.count('&')\n",
        "\n",
        "def count_star(url):\n",
        "    return url.count('*')\n",
        "\n",
        "def count_left_parenthesis(url):\n",
        "    return url.count('(')\n",
        "\n",
        "def count_right_parenthesis(url):\n",
        "    return url.count(')')\n",
        "\n",
        "def count_hyphens(url):\n",
        "    return url.count('-')\n",
        "\n",
        "def count_underscore(url):\n",
        "    return url.count('_')\n",
        "\n",
        "def count_plus(url):\n",
        "    return url.count('+')\n",
        "\n",
        "def count_equal(url):\n",
        "    return url.count('=')\n",
        "\n",
        "def count_left_curly_brace(url):\n",
        "    return url.count('{')\n",
        "\n",
        "def count_right_curly_brace(url):\n",
        "    return url.count('}')\n",
        "\n",
        "def count_left_square_bracket(url):\n",
        "    return url.count('[')\n",
        "\n",
        "def count_right_square_bracket(url):\n",
        "    return url.count(']')\n",
        "\n",
        "def count_or(url):\n",
        "    return url.count('|')\n",
        "\n",
        "def count_backslash(url):\n",
        "    return url.count('\\\\')\n",
        "\n",
        "def count_colon(url):\n",
        "    return url.count(':')\n",
        "\n",
        "def count_semicolon(url):\n",
        "    return url.count(';')\n",
        "\n",
        "def count_comma(url):\n",
        "    return url.count(',')\n",
        "\n",
        "def count_dot(url):\n",
        "    return url.count('.')\n",
        "\n",
        "def count_less_than(url):\n",
        "    return url.count('<')\n",
        "\n",
        "def count_greater_than(url):\n",
        "    return url.count('>')\n",
        "\n",
        "def count_question(url):\n",
        "    return url.count('?')\n",
        "\n",
        "def count_slash(url):\n",
        "    return url.count('/')\n",
        "\n",
        "def count_double_slash(url):\n",
        "    if \"//\" in url:\n",
        "        result = [x.start(0) for x in re.finditer('//', url)]\n",
        "        return 1 if result[len(result) - 1] > 6 else 0\n",
        "    return 0\n",
        "\n",
        "def count_space(url):\n",
        "    return url.count(' ')\n",
        "\n",
        "def count_digits(url):\n",
        "    return len(re.sub(\"[^0-9]\", \"\", url))\n",
        "\n",
        "def count_letters(url):\n",
        "    return len(re.sub(\"[^a-zA-Z]\", \"\", url))\n",
        "\n",
        "def digits_ratio(url):\n",
        "    return count_digits(url) / len(url)\n",
        "\n",
        "def letters_ratio(url):\n",
        "    return count_letters(url) / len(url)\n",
        "\n",
        "def https_token(scheme):\n",
        "    return 1 if scheme == 'https' else 0\n",
        "\n",
        "def count_http_token(url_path):\n",
        "    return url_path.count('http')\n",
        "\n",
        "def tld_in_path(tld, url_path):\n",
        "    return 1 if url_path.lower().count(tld) > 0 else 0\n",
        "\n",
        "def brand_in_path(domain, url_path):\n",
        "    for brand in allBrand:\n",
        "        brand = brand.lower().replace(\" \", \"\")\n",
        "        url_path = url_path.lower()\n",
        "        if brand in url_path and brand not in domain:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def tld_in_subdomain(tld, subdomain):\n",
        "    return 1 if subdomain.lower().count(tld) > 0 else 0\n",
        "\n",
        "def brand_in_domain(domain):\n",
        "    return 1 if domain in allBrand else 0\n",
        "\n",
        "def misspelling_brand_in_domain(domain):\n",
        "    for brand in allBrand:\n",
        "        brand = brand.lower().replace(\" \", \"\")\n",
        "        domain = domain.lower()\n",
        "        if len(Levenshtein.editops(domain, brand)) < 2:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def puny_code(url):\n",
        "    return 1 if url.startswith(\"http://xn--\") or url.startswith(\"https://xn--\") else 0\n",
        "\n",
        "def char_repeat(words_raw):\n",
        "    def all_same(items):\n",
        "        return all(x == items[0] for x in items)\n",
        "    repeat = {'2': 0, '3': 0, '4': 0, '5': 0}\n",
        "    part = [2, 3, 4, 5]\n",
        "    for word in words_raw:\n",
        "        for char_repeat_count in part:\n",
        "            for i in range(len(word) - char_repeat_count + 1):\n",
        "                sub_word = word[i: i + char_repeat_count]\n",
        "                if all_same(sub_word):\n",
        "                    repeat[str(char_repeat_count)] += 1\n",
        "    return sum(list(repeat.values()))\n",
        "\n",
        "def count_www(words_raw):\n",
        "    result = 0\n",
        "    for word in words_raw:\n",
        "        if word.find('www') != -1:\n",
        "            result += 1\n",
        "    return result\n",
        "\n",
        "def count_com(words_raw):\n",
        "    result = 0\n",
        "    for word in words_raw:\n",
        "        if word.find('com') != -1:\n",
        "            result += 1\n",
        "    return result\n",
        "\n",
        "def port(url):\n",
        "    if re.search(\"^[a-z][a-z0-9+\\-.]*://([a-z0-9\\-._~%!$&'()*+,;=]+@)?([a-z0-9\\-._~%]+|\\[[a-z0-9\\-._~%!$&'()*+,;=:]+\\]):([0-9]+)\",url):\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def length_word_raw(words_raw):\n",
        "    return len(words_raw)\n",
        "\n",
        "\n",
        "def average_word_length(words_raw):\n",
        "    if len(words_raw) == 0:\n",
        "        return 0\n",
        "    return sum([len(word) for word in words_raw]) / len(words_raw)\n",
        "\n",
        "def longest_word_length(words_raw):\n",
        "    if len(words_raw) == 0:\n",
        "        return 0\n",
        "    return max([len(word) for word in words_raw])\n",
        "\n",
        "def shortest_word_length(words_raw):\n",
        "    if len(words_raw) == 0:\n",
        "        return 0\n",
        "    return min([len(word) for word in words_raw])\n",
        "\n",
        "def prefix_suffix(url):\n",
        "    return 1 if re.findall(r\"https?://[^\\-]+-[^\\-]+/\", url) else 0\n",
        "\n",
        "def count_subdomain(url):\n",
        "    extracted_info = tldextract.extract(url)\n",
        "\n",
        "    combined_domain = f\"{extracted_info.subdomain}.{extracted_info.domain}\"\n",
        "\n",
        "    subdomain_count = len(combined_domain.split('.'))\n",
        "\n",
        "    return subdomain_count\n",
        "\n",
        "# url_to_test = \"https://www.baltazarpresentes.com.br/example\"\n",
        "# domain_to_test = \"baltazarpresentes.com.br\"\n",
        "# result == 1 => Security threat detected!\n",
        "# result == 0 => No security threat detected\n",
        "# result == 2 => Error during the security check\n",
        "def statistical_report(url, domain):\n",
        "    url_match = re.search('at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly',url)\n",
        "    try:\n",
        "        ip_address = socket.gethostbyname(domain)\n",
        "        ip_match = re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|'\n",
        "                           '107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|'\n",
        "                           '118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|'\n",
        "                           '216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|'\n",
        "                           '34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|'\n",
        "                           '216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42',ip_address)\n",
        "        if url_match or ip_match:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    except:\n",
        "        return 2\n",
        "\n",
        "suspicious_tlds = ['fit','tk', 'gp', 'ga', 'work', 'ml', 'date', 'wang', 'men', 'icu', 'online', 'click', # Spamhaus\n",
        "        'country', 'stream', 'download', 'xin', 'racing', 'jetzt',\n",
        "        'ren', 'mom', 'party', 'review', 'trade', 'accountants',\n",
        "        'science', 'work', 'ninja', 'xyz', 'faith', 'zip', 'cricket', 'win',\n",
        "        'accountant', 'realtor', 'top', 'christmas', 'gdn', # Shady Top-Level Domains\n",
        "        'link', # Blue Coat Systems\n",
        "        'asia', 'club', 'la', 'ae', 'exposed', 'pe', 'go.id', 'rs', 'k12.pa.us', 'or.kr',\n",
        "        'ce.ke', 'audio', 'gob.pe', 'gov.az', 'website', 'bj', 'mx', 'media', 'sa.gov.au'] # statistics\n",
        "\n",
        "\n",
        "def suspicious_tld(tld):\n",
        "    return 1 if tld in suspicious_tlds else 0\n",
        "\n",
        "def count_phishing_hints(url_path):\n",
        "    result = 0\n",
        "    for hint in HINTS:\n",
        "        result += url_path.lower().count(hint)\n",
        "    return result\n",
        "\n",
        "def abnormal_subdomain(url):\n",
        "    return 1 if re.search('(http[s]?://(w[w]?|\\d))([w]?(\\d|-))',url) else 0\n",
        "\n",
        "def path_extension(url_path):\n",
        "    malicious_extensions = ['.txt', '.exe', '.js']\n",
        "    file_extension = splitext(url_path)[1].lower()\n",
        "    return 1 if file_extension in malicious_extensions else 0\n",
        "\n",
        "def shortening_service(url):\n",
        "    match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
        "                      'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
        "                      'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
        "                      'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
        "                      'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
        "                      'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
        "                      'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|'\n",
        "                      'tr\\.im|link\\.zip\\.net',\n",
        "                      url)\n",
        "    return 1 if match else 0\n",
        "\n",
        "def having_ip_address(url):\n",
        "    match = re.search(\n",
        "        '(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.'\n",
        "        '([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  # IPv4\n",
        "        '(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.'\n",
        "        '([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  # IPv4 with port\n",
        "        '((0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\/)'  # IPv4 in hexadecimal\n",
        "        '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}|'\n",
        "        '([0-9]+(?:\\.[0-9]+){3}:[0-9]+)|'\n",
        "        '((?:(?:\\d|[01]?\\d\\d|2[0-4]\\d|25[0-5])\\.){3}(?:25[0-5]|2[0-4]\\d|[01]?\\d\\d|\\d)(?:\\/\\d{1,2})?)', url)  # Ipv6\n",
        "    return 1 if match else 0\n",
        "\n",
        "def is_domain_expired(url):\n",
        "    try:\n",
        "        domain_info = whois.whois(url)\n",
        "        expiration_date = domain_info.expiration_date\n",
        "\n",
        "        if isinstance(expiration_date, list):\n",
        "            expiration_date = expiration_date[0]\n",
        "\n",
        "        if expiration_date:\n",
        "            current_date = datetime.now()\n",
        "            return current_date > expiration_date\n",
        "    except whois.parser.PywhoisError as e:\n",
        "        return 1\n",
        "\n",
        "def get_domain_age(url):\n",
        "    try:\n",
        "        domain_info = whois.whois(url)\n",
        "        creation_date = domain_info.creation_date\n",
        "        expiration_date = domain_info.expiration_date\n",
        "\n",
        "        if isinstance(creation_date, list):\n",
        "            creation_date = creation_date[0]\n",
        "\n",
        "        if creation_date:\n",
        "            current_date = datetime.now()\n",
        "            age = current_date - creation_date\n",
        "            return age.days\n",
        "    except whois.parser.PywhoisError as e:\n",
        "        return -1\n",
        "\n",
        "def check_redirects(url):\n",
        "    try:\n",
        "        response = requests.head(url, allow_redirects=True)\n",
        "        final_url = response.url\n",
        "        if url != final_url:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    except requests.RequestException as e:\n",
        "        return 2\n",
        "\n",
        "def words_raw_extraction(domain, subdomain, url_path):\n",
        "    w_domain = re.split(\"\\-|\\.|\\/|\\?|\\=|\\@|\\&|\\%|\\:|\\_\", domain.lower())\n",
        "    w_subdomain = re.split(\"\\-|\\.|\\/|\\?|\\=|\\@|\\&|\\%|\\:|\\_\", subdomain.lower())\n",
        "    w_url_path = re.split(\"\\-|\\.|\\/|\\?|\\=|\\@|\\&|\\%|\\:|\\_\", url_path.lower())\n",
        "    raw_words = w_domain + w_url_path + w_subdomain\n",
        "    w_host = w_domain + w_subdomain\n",
        "    raw_words = list(filter(None, raw_words))\n",
        "    return raw_words, list(filter(None, w_host)), list(filter(None, w_url_path))\n",
        "\n",
        "\n",
        "def featureExtraction(data):\n",
        "    count = 0\n",
        "    data['decoded_url'] = data[\"url\"].apply(lambda x: urllib.parse.unquote(x))\n",
        "    data[\"scheme\"] = data[\"url\"].apply(lambda x: urlparse(x).scheme)\n",
        "    data['domain'] = data['url'].apply(lambda x: get_domain_from_url(x))\n",
        "    data[\"subdomain\"] = data[\"url\"].apply(lambda x: tldextract.extract(x).subdomain)\n",
        "    data[\"second_domain\"] = data[\"url\"].apply(lambda x: tldextract.extract(x).domain)\n",
        "    data[\"tld\"] = data[\"url\"].apply(lambda x: tldextract.extract(x).suffix)\n",
        "    data[\"url_path\"] = data[\"url\"].apply(lambda x: urlparse(x).path)\n",
        "    data[\"words_raw\"] = data.apply(lambda x: words_raw_extraction(x[\"url\"], x[\"subdomain\"], x[\"url_path\"]), axis=1)\n",
        "\n",
        "    data[url_length.__name__] = data['url'].apply(lambda x: url_length(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[hostname_length.__name__] = data['url'].apply(lambda x: hostname_length(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_at.__name__] = data['decoded_url'].apply(lambda x: count_at(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_hash.__name__] = data['decoded_url'].apply(lambda x: count_hash(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_dollar.__name__] = data['decoded_url'].apply(lambda x: count_dollar(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_percentage.__name__] = data['decoded_url'].apply(lambda x: count_percentage(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_and.__name__] = data['decoded_url'].apply(lambda x: count_and(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_star.__name__] = data['decoded_url'].apply(lambda x: count_star(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_exclamation.__name__] = data['decoded_url'].apply(lambda x: count_exclamation(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_caret.__name__] = data['decoded_url'].apply(lambda x: count_caret(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_left_parenthesis.__name__] = data['decoded_url'].apply(lambda x: count_left_parenthesis(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_right_parenthesis.__name__] = data['decoded_url'].apply(lambda x: count_right_parenthesis(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_hyphens.__name__] = data['decoded_url'].apply(lambda x: count_hyphens(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_underscore.__name__] = data['decoded_url'].apply(lambda x: count_underscore(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_plus.__name__] = data['decoded_url'].apply(lambda x: count_plus(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_hash.__name__] = data['decoded_url'].apply(lambda x: count_hash(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_equal.__name__] = data['decoded_url'].apply(lambda x: count_equal(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_left_curly_brace.__name__] = data['decoded_url'].apply(lambda x: count_left_curly_brace(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_right_curly_brace.__name__] = data['decoded_url'].apply(lambda x: count_right_curly_brace(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_left_square_bracket.__name__] = data['decoded_url'].apply(lambda x: count_left_square_bracket(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_right_square_bracket.__name__] = data['decoded_url'].apply(lambda x: count_right_square_bracket(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_or.__name__] = data['decoded_url'].apply(lambda x: count_or(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_backslash.__name__] = data['decoded_url'].apply(lambda x: count_backslash(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_colon.__name__] = data['decoded_url'].apply(lambda x: count_colon(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_semicolon.__name__] = data['decoded_url'].apply(lambda x: count_semicolon(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_tilde.__name__] = data['decoded_url'].apply(lambda x: count_tilde(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_comma.__name__] = data['decoded_url'].apply(lambda x: count_comma(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_dot.__name__] = data['decoded_url'].apply(lambda x: count_dot(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_less_than.__name__] = data['decoded_url'].apply(lambda x: count_less_than(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_greater_than.__name__] = data['decoded_url'].apply(lambda x: count_greater_than(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_question.__name__] = data['decoded_url'].apply(lambda x: count_question(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_slash.__name__] = data['decoded_url'].apply(lambda x: count_slash(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_double_slash.__name__] = data['decoded_url'].apply(lambda x: count_double_slash(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_space.__name__] = data['decoded_url'].apply(lambda x: count_space(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_digits.__name__] = data['decoded_url'].apply(lambda x: count_digits(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_letters.__name__] = data['decoded_url'].apply(lambda x: count_letters(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[digits_ratio.__name__] = data['decoded_url'].apply(lambda x: digits_ratio(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[letters_ratio.__name__] = data['decoded_url'].apply(lambda x: letters_ratio(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[https_token.__name__] = data['scheme'].apply(lambda x: https_token(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_http_token.__name__] = data['url_path'].apply(lambda x: count_http_token(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[tld_in_path.__name__] = data.apply(lambda x: tld_in_path(x['tld'], x['url_path']), axis=1)\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[brand_in_path.__name__] = data.apply(lambda x: brand_in_path(x['second_domain'], x['url_path']), axis=1)\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[tld_in_subdomain.__name__] = data.apply(lambda x: tld_in_subdomain(x['tld'], x['subdomain']), axis=1)\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[brand_in_domain.__name__] = data.apply(lambda x: brand_in_domain(x['second_domain']), axis=1)\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[misspelling_brand_in_domain.__name__] = data.apply(lambda x: misspelling_brand_in_domain(x['second_domain']), axis=1)\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[puny_code.__name__] = data['url'].apply(lambda x: puny_code(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[char_repeat.__name__] = data[\"words_raw\"].apply(lambda x: char_repeat(x[0]))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_www.__name__] = data[\"words_raw\"].apply(lambda x: count_www(x[0]))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_com.__name__] = data[\"words_raw\"].apply(lambda x: count_com(x[0]))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[port.__name__] = data[\"url\"].apply(lambda x: port(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[length_word_raw.__name__] = data[\"words_raw\"].apply(lambda x: length_word_raw(x[0]))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[average_word_length.__name__] = data[\"words_raw\"].apply(lambda x: average_word_length(x[0]))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[longest_word_length.__name__] = data[\"words_raw\"].apply(lambda x: longest_word_length(x[0]))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[shortest_word_length.__name__] = data[\"words_raw\"].apply(lambda x: shortest_word_length(x[0]))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[prefix_suffix.__name__] = data[\"url\"].apply(lambda x: prefix_suffix(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_subdomain.__name__] = data[\"url\"].apply(lambda x: count_subdomain(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    # data[statistical_report.__name__] = data.apply(lambda x: statistical_report(x['url'], x['domain']), axis=1)\n",
        "    # count += 1\n",
        "    # print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[suspicious_tld.__name__] = data[\"tld\"].apply(lambda x: suspicious_tld(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[count_phishing_hints.__name__] = data[\"url_path\"].apply(lambda x: count_phishing_hints(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[abnormal_subdomain.__name__] = data[\"url\"].apply(lambda x: abnormal_subdomain(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[path_extension.__name__] = data[\"url_path\"].apply(lambda x: path_extension(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[shortening_service.__name__] = data[\"url\"].apply(lambda x: shortening_service(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    data[having_ip_address.__name__] = data[\"url\"].apply(lambda x: having_ip_address(x))\n",
        "    count += 1\n",
        "    print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    # data[is_domain_expired.__name__] = data[\"url\"].apply(lambda x: is_domain_expired(x))\n",
        "    # count += 1\n",
        "    # print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    # data[get_domain_age.__name__] = data[\"url\"].apply(lambda x: get_domain_age(x))\n",
        "    # count += 1\n",
        "    # print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    # data[check_redirects.__name__] = data[\"url\"].apply(lambda x: check_redirects(x))\n",
        "    # count += 1\n",
        "    # print(\"Completed Extraction: \" + str(count) + \"/65\")\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "5LthOYKKEzLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "DATASET_URL = \"/gdrive/MyDrive/Colab Notebooks/1M-PD/newDatasets.csv\"\n",
        "\n",
        "def preprocessing_DATA():\n",
        "    data = pd.read_csv(DATASET_URL, sep=\",\", encoding=\"utf-8\")\n",
        "    no_label_index = list(\n",
        "        data.loc[(data['label'] != '1') & (data['label'] != 0) & (data['label'] != '0') & (data['label'] != 1)].index)\n",
        "    data = data.drop(index=no_label_index)\n",
        "    data = data.reset_index(drop=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "AkFiU0jDQQaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = preprocessing_DATA()\n",
        "data = featureExtraction(data)\n",
        "data = data.drop(columns=['decoded_url'], axis=1)\n",
        "data.to_csv('/gdrive/MyDrive/Colab Notebooks/1M-PD/train_.csv', encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "cL6BopyQR8rq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}